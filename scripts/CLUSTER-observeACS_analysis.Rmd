---
title: "Observe-ACS"
author: | 
        | christoph.reich@med.uni-heidelberg.de 
        | evangelos.giannitsis@med.uni-heidelberg.de
        | Klinik für Kardiologie, Angiologie und Pneumologie
        | Universitätsklinikum Heidelberg
        | 
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    df_print: kable
---


```{r dependencies, include=FALSE}
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(tableone)
library(skimr)
library(mice)  # for missing data
library(VIM)  # for missing data
library(caret)  # for modelling
library(tictoc)
library(pROC)
library(glmnet)
library(survival)
library(survminer)
library(ggsci)
library(lubridate)

# define mode function (will be needed later)  !!!
mode <- function(x, na.rm = FALSE) {
  if(na.rm){
    x = x[!is.na(x)]
  }

  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}

```


```{r read-data, echo=FALSE}
base::source("scripts/data_preprocessing.R")
```

![AG Meder](../../img//ukhd/meder_presentation_heart.png)

# Data description

The data set consists of `r nrow(dat.observe)` patients. There are `r ncol(dat.observe)` variables in our data set.
`r round(100*(sum(is.na(dat.observe)) / ncol(dat.observe) / nrow(dat.observe)), 1)`\% of the 
values are missing.

# Demographics

## EDA 

First we want to describe our *ObserveACS*-cohort and get an impression of the variable's distribution: 

```{r EDA, echo=FALSE}
# skim
skimr::skim(data=dat.observe)

# vector of nonnormal vars
## nonnormal <-  c("ntproBNP", "Trop", "Kreatinin", "Triglycerides", "LGE")
```


### inspect some distributions

* age

```{r inspect age distr, echo=FALSE, fig.show="hold", out.width="50%"}
ggplot2::ggplot(data=dat.observe, aes(x=V_age))+
  geom_histogram(colour="black", fill="white", bins = 30)+
  xlab("Age")+
  ylab("Count")+
  theme_bw()

ggplot2::ggplot(data=dat.observe, aes(x=V_age))+
  geom_histogram(aes(y=..density..), colour="black", fill="white", bins = 30)+
    geom_density(alpha=0.2, fill="#FF6666")+
  xlab("Age")+
  ylab("Density")+
  theme_bw()
```

* copeptin

```{r inspect copeptin distr, echo=FALSE, fig.show="hold", out.width="50%"}
ggplot2::ggplot(data=dat.observe, aes(x=COPEPTIN_COMBINED))+
  geom_histogram(colour="black", fill="white", bins = 30, na.rm = TRUE)+
  xlab("Copeptin")+
  ylab("Count")+
  theme_bw()

ggplot2::ggplot(data=dat.observe, aes(x=COPEPTIN_COMBINED))+
  geom_histogram(aes(y=..density..), colour="black", fill="white", bins = 30, na.rm=TRUE)+
  geom_density(alpha=0.2, fill="#FF6666", na.rm=TRUE)+
  xlab("Copeptin")+
  ylab("Density")+
  theme_bw()
```

### inspect all numeric vars' distributions

```{r loop numeric cols for distr, echo=FALSE, fig.show="hold", out.width="50%"}
colNames <- names(dat.observe_numeric)[2:47]

for(i in colNames){
  plt <- ggplot(dat.observe_numeric, aes_string(x=i)) +
    geom_histogram(aes(y=..density..), 
                   colour="black", fill="white", bins = 30, na.rm=TRUE)+
    geom_density(alpha=0.2, fill="#FF6666", na.rm = TRUE)+
    xlab(i)+
    ylab("Density")+
    theme_bw()
  print(plt)
  # Sys.sleep(2) # time to inspect plot
}
```

We can clearly observe that not all vars follow a Gaussian distribution. We might
consider this in our descriptive stats. So we create a vector ob clearly non-normal
features: 

```{r, nonnormality check for descriptive stats, echo=FALSE}
# vector of nonnormal vars
#sunonnormal <-  c("ntproBNP", "Trop", "Kreatinin", "Triglycerides", "LGE")

```

\clearpage

# Missing values

## Inspect missing values

Here we inspect missing values. Later we deal with missing values with the `MICE` approach. First we calculate which cols are missing more than 30% data: 

```{r investigate-NA, echo=FALSE}
# show vals that have > 30% missing data
pMiss_observe <- apply(dat.observe,2,pMiss)
index <- pMiss_observe > 30
round(pMiss_observe[index], 2)
```

We can conclude that some vals are just missing because they were not existing (e.g., `V_ekg_st_hebung`
`V_ekg_schenkelblock`, `V_o_stroke`, `V_o_time_stroke`, ...). Therefore, we go on and add `0` for some vars
where we can be quite sure (done in script **data_preprocessing.R** and stored in `dat`).


## Plot missing data

```{r plot missing NA, echo=FALSE}
# md.pattern(dat.observe, plot=FALSE)

dat.observe_numeric %>% select(-V_rapID) %>% 
  select(1:10) %>% 
  mice::md.pattern(.)

aggr_plot_demo <- VIM::aggr(dat.observe[demographics_cat], 
                           col=c('navyblue','red'), 
                           numbers=TRUE, 
                           sortVars=TRUE, 
                           labels=substring(names(dat.observe[demographics_cat]), 5),
                           cex.axis=.7, gap=3, 
                           ylab=c("Histogram of missing data","Pattern"),
                           combined=FALSE
)

aggr_plot_ecg <- VIM::aggr(dat.observe[ecg_cat], 
                       col=c('navyblue','red'), 
                       numbers=TRUE, 
                       sortVars=TRUE, 
                       labels=substring(names(dat.observe[ecg_cat]), 7),
                       cex.axis=.7, gap=3, 
                       ylab=c("Histogram of missing data","Pattern"),
                       combined=FALSE
)

aggr_plot_outcomes <- VIM::aggr(dat.observe[outcomes_cat], 
                           col=c('navyblue','red'), 
                           numbers=TRUE, 
                           sortVars=TRUE, 
                           labels=substring(names(dat.observe[outcomes_cat]), 2),
                           cex.axis=.7, gap=3, 
                           ylab=c("Histogram of missing data","Pattern"),
                           combined=FALSE
)

aggr_plot_scores <- VIM::aggr(dat.observe[scores_cat], 
                                col=c('navyblue','red'), 
                                numbers=TRUE, 
                                sortVars=TRUE, 
                                labels=substring(names(dat.observe[scores_cat]), 10),
                                cex.axis=.7, gap=3, 
                                ylab=c("Histogram of missing data","Pattern"),
                                combined=FALSE
)

```

## Drop cols missing more >30% data

```{r reinvestigate-NA-after-edit, echo=FALSE}
# edit NA values (also done in preprocessing and stored in 'dat'), here again to show what was done
dat.observe %>%
  # replace_na works with "specified values" only
  tidyr::replace_na(
    list(
      V_ekg_st_hebung = 0,
      V_ekg_schenkelblock = 0,
      V_o_stroke = 0,
      V_o_cabg=0,
      V_o_reinfarction=0,
      V_o_recoro=0,
      V_o_re_ptca =0,
      # # we also have to edit the outcome data and assume that patients where NA is observed in mortality -> no mortality
      V_mortality_30d = 0,
      V_mortality_90d=0,
      V_o_mortality=0,
      V_aufnahme_krankenhaus=0
    )
  ) %>% 
  # to replace with values from column we use mutate and ifelse
  # here we can add time until death as those patients definitely were not
  # observed longer and would not be counted as stroke
  mutate(
    V_o_time_stroke = ifelse(is.na(V_o_time_stroke),V_o_time_mortality, V_o_time_stroke),
    V_o_time_cabg = ifelse(is.na(V_o_time_cabg),V_o_time_mortality, V_o_time_cabg),
    V_o_time_reinfarction = ifelse(is.na(V_o_time_reinfarction),V_o_time_mortality, V_o_time_reinfarction),
    V_o_time_recoro = ifelse(is.na(V_o_time_recoro),V_o_time_mortality, V_o_time_recoro),
    V_o_time_re_ptca = ifelse(is.na(V_o_time_re_ptca),V_o_time_mortality, V_o_time_re_ptca)
  ) -> dat.observe

pMiss_observe <- apply(dat.observe,2,pMiss)
index <- pMiss_observe > 30
```

Now we continue and drop cols with missing > 30% data (containing at least 70%; done in `data_preprocessing.R`
and stored in `dat`). Those are the following cols (n=`r sum(index)`): <br/>

`r kableExtra::kable(names(pMiss_observe[index]))`


# Data description after NA edit

The data set consists of `r nrow(dat)` patients. 
After removing variables with too many missing values, 
`r ncol(dat)` variables remain in the data set.
Now, `r round(100*(sum(is.na(dat)) / ncol(dat) / nrow(dat)), 1)`\% of the 
values are missing.

\clearpage


# MICE Imputation

## Predictor Matrix

We use the mice package for multiple imputation.  
*CAVE*: If there are too many factor levels -> `mice` automatically converts factors 
(categorical variables) to dummy variables, each level is modeled by its own imputation mode! <br /> 

The default methods are:

- `pmm`: predictive mean matching for numeric and integer data
- `logreg`: log regression for binary data 
- `polyreg`: polytomous regression imputation for un- ordered categorical data (factor > 2 levels)
- `polr`: POM for orderd, > 2 levels

```{r separate-outcomes-from-dat, echo=FALSE}
# dat was created in data_preprocessing

# dat.outcome

# dat.independent
```

We created a separate dataframe only consisting of `r ncol(dat.independent)` independent predictor vars, 
on which we are going to perform multiple imputation. First, we define which vars we do not want to 
include in our `predictorMatrix` for multiple imputation (remove vars as mice predictors). We can also
define vars we want to exclude from imputation. Since we have already separated the **outcomes dataframe**, 
we can carry on without trouble ;)

```{r set-imp-vals, echo=FALSE}
set.seed(123)
# no of imputations
m <- 100

# initiate matrices --------------------------------------------------------
init = mice(dat.independent, maxit=0) 
meth = init$method
predM = init$predictorMatrix

# remove variables as predictors -------------------------------------------
predM[, c("V_rapID", "V_admission_month", "V_admission_week","V_admission_weekday","V_admission_hour","V_symptombeginn","V_symptom_thoraxschmerz","V_symptomtyp","V_h_vessel_disease","V_h_lvdys_grad","V_ekg_t_negativierung","V_ekg_schrittmacher","V_ekg_atriale_tachy","V_ekg_block_ohne_sm","V_ekg_st_senkung","V_ekg_sinusrhythmus","V_ekg_sinus_normal","V_ekg_st_hebung","KHK__Killip_Class")
      ]=0

# skip var from imputation -----------------------------------------------
# meth[c("Age")]=""

# specify alternative method ----------------------------------------------
# meth[c("Cholesterol")]="norm" 
# meth[c("Education")]="polr"  # POM model

# logged events -----------------------------------
head(init$loggedEvents, 2)
```

We receive warning on **logged events**: mice found some peculiarities in the data that need the user’s attention. The logged events form a structured report that identify problems with the data, and details which corrective actions were taken by mice(). It is a component called loggedEvents of the mids object. A constant variable is removed from the imputation model (unless the `remove.constant=FALSE` argument is specified); a variable that is collinear with another variable is removed from the imputation model, unless the `remove.collinear=FALSE` argument is specified. 

Categorical vars are internally represented as dummy vars, so the actual number of predictors can easily explode! This makes the algorithm slow, if it runs at all.

```{r inspectNAagain, echo=FALSE}
table(init$nmis)  # 26 vars are complete
```

We can see that `r table(init$nmis)[1]` vars are complete.

## Imputation Process

Now it is time to run the multiple (m=`r m`) imputation.

```{r imputation, include=FALSE}
library(tictoc)
tictoc::tic()

# run the multiple imputation:
imputed <- mice::mice(dat.independent, 
                     m = m,  # no of multiple imputations
                     method = meth,  
                     predictorMatrix = predM,  # specifying the set of predictors to be used for each target column
                     printFlag = F,  # for silent computation
                     seed = 2022)

tictoc::toc()  # for m=5 ~37.827 sec elapsed

# Create a dataset after imputation:
complete(imputed, 
         action="long",  # m imputed datasets 
         include = FALSE
         )  %>%
  # check logged events again (would be excluded if collinearity occurs) https://stefvanbuuren.name/fimd/sec-toomany.html
  select(-c(imputed$loggedEvents$out[imputed$loggedEvents$meth == "collinear"])) %>%
  # check that no more missing vars are in the data
  select_if(~sum(is.na(.)) == 0)  -> impdat
```


## Inspect imputation

Inspect the trace lines: 

```{r plot-trace, echo=FALSE}
plot(imputed)
stripplot(imputed)
```


# Correlation of predictor vars

E.g. we can look at systolic and diastolic blood pressure:

```{r correlation, echo=FALSE}
# blood pressure 
cor(dat.independent$V_vit_rr_syst, y=dat.independent$V_vit_rr_diast, use="complete.obs")

## ALL numeric Vars ----------------------------------------------------------
# only numeric vars for correlation
numcols <- unlist(lapply(impdat, is.numeric))
cordat <- impdat[numcols][4:33]
# calculate correlation matrix
correlationMatrix <- cor(x=cordat, method = "pearson")
highCorr <- sum(abs(correlationMatrix[base::upper.tri(correlationMatrix)]) > .75)  # 5 vars can later be dropped
summary(correlationMatrix[upper.tri(correlationMatrix)])

# find attributes that are highly corrected (ideally >0.75) -------------------
highlyCorrelated <- caret::findCorrelation(correlationMatrix, cutoff=0.75, verbose=TRUE,
                                           #names=TRUE, 
                                           exact = TRUE) 
# Generally, you want to remove attributes with an absolute correlation of 0.75 or higher.
# print indexes of highly correlated attributes 
print(highlyCorrelated)
# names of highly correlated features in "cordat" (correlationMatrix is matrix -> no names) -----
colnames(cordat)[highlyCorrelated]  # e.g., ck correlated with krea 

# FIND OUT which vars are correlated? -> look at output (verbose=TRUE) of findCorrelation
## 1) "Compare row 10  and column  11 with corr  0.808";   
colnames(cordat)[10:11]  # ckdepi V_t0_krea
# 2) Compare row 22  and column  23 with corr  0.964
colnames(cordat)[22:23]  # Hb und Hämatokrit
# 3) Compare row 28  and column  29 with corr  0.761 
colnames(cordat)[28:29]  # "V_t0_quick_value" "V_t0_inr_value"
# 4) Compare row 24  and column  25 with corr  0.863
colnames(cordat)[24:25]  # "V_t0_mcv_value" "V_t0_mch_value"
# 5) Compare row 20  and column  21 with corr  0.785
colnames(cordat)[20:21]  # ""V_t0_got_value" "V_t0_gpt_value"

# plot heatmap - for ggcorr and corrplot too many features
heatmap(x = correlationMatrix, symm = TRUE)
```

## High Correlation plots

```{r plot-high-cor-features, echo=FALSE}
library(PerformanceAnalytics)
my_data <- cordat[c(10:11, 22:23, 28:29, 24:25, 20:21)]
PerformanceAnalytics::chart.Correlation(my_data, histogram=TRUE, pch=19)

library(GGally)
GGally::ggpairs(my_data, title="correlogram") 
```

\clearpage


# Feature Selection

## Elastic Net

We have to do variable selection for each imputed dataset separately!!!!!
For **each** dataset, we search the optimal elastic net penalty parameters
($\alpha$ and $\lambda$) via grid search.

### Overall Mortality

From `nrow(dat)` patients in our *Observe-ACS* cohort we observed overall mortality in `r sum(dat.outcome$V_o_mortality==1)` patients whereas `r sum(dat.outcome$V_o_mortality==0)` survived. 

```{r elastic-net-overall-mortality, echo=FALSE}

set.seed(20220106)
# for cross-validation
K=5

# impdat %>%
#   # only interested in pat without Afib, since that is an indication for NOAC per se
#   filter(basis_afib == "0") %>%
#   # drop column, since no information (only zeros)
#   select(-basis_afib) -> dat2

ctrl <- caret::trainControl(method = "cv",
                     number = K,  
                     classProbs = T,
                     summaryFunction = caret::twoClassSummary)

enet_grid <- expand.grid(alpha = seq(0.1, 0.9, 0.1),
                         lambda = c(0.1, 0.2, 0.5))

# create matrix to store beta values with nrow=m, and ncol=ncol(independent_data)+auc
beta_overall <- matrix(nrow = m, ncol = ncol(impdat) - 2)  # we do not need .imp, .id, V_rapID in our matrix (but we will add "Best_AUC")
colnames(beta_overall) <- c(names(impdat[,-c(1,2,3)]), "Best_AUC")

# iterate over all multiple imputed datasets 1:m ---------------------------
tictoc::tic()
for(i in 1:m) {
  ## independent data "X"
  impdat %>% 
    dplyr::filter(.imp == i) %>%  # filter imputed dataset 
    # drop unnecessary cols (decide what we want to model)
    dplyr::select(-c(.id, .imp, V_rapID)) %>% 
    # convert to data.matrix necessary for caret
    base::data.matrix(.) %>% 
    # scale data
    base::scale(x=., center=TRUE, scale=TRUE) -> X
          # .imp An optional column number or column name in long, indicating the imputation
          # index. The values are assumed to be consecutive integers between 0 and m.
          # Values 1 through m correspond to the imputation index, value 0 indicates the
          # original data (with missings). By default, the procedure will search for a variable
          # named ".imp".
  ## target data "y"
  dat.outcome %>% 
    # we do not have multiple imputed outcome data (no need to filter(.imp == i))
    select(V_o_mortality) %>%
    # unlist needed for caret
    unlist %>%
    # rename factor labels
    factor(., labels = c("no", "yes")) -> y
  
  plr1 <- caret::train(x=X, 
                       y=y, 
                       method = "glmnet", 
                       metric = "ROC", 
                       tuneGrid = enet_grid,
                       trControl = ctrl)
  
  net1 <- glmnet(X, 
                 as.factor(y),
                 family = "binomial",
                 alpha = plr1$bestTune$alpha, # tune!
                 lambda = plr1$bestTune$lambda) # tune!
  
  beta_overall[i, ] <- c(as.numeric(net1$beta), max(plr1$results$ROC))
}
tictoc::toc()

# add mean of all imputation values, min, max and colSums where beta !=0
beta_overall <- rbind(beta_overall,  # beta values (for m imputations m rows)
              colMeans(beta_overall),  # mean of beta values across imputations
              apply(beta_overall, 2, min),  # min beta across imputations
              apply(beta_overall, 2, max),  # max beta across imputations
              colSums(beta_overall != 0)  # how often was feature chosen? so where is beta not null and sumup
              )

beta_overall <- base::t(beta_overall)  # matrix transpose
# order matrix based on how hoften betas were calculated
index <- rev(
  order(  # Ordering Permutation -> returns index
    beta_overall[,ncol(beta_overall)]  # order based on last col of beta (which is how often a feature was chosen (has beta values))
    )
  )
# order beta matrix
beta_overall <- beta_overall[index,]
# delete all the beta estimates based on one imputation only
beta_overall <- beta_overall[,-(1:m)]
colnames(beta_overall) <- c("Mean Estimate", "Minimum", "Maximum", "Selection Count") # count how often each feature was selected in imputed datasets 
```

We calculate our estimates based on `r length(unique(impdat$.id))` patients in the ACS observe zone.  

We obtain

```{r beta-overall-mortality, echo=FALSE}
round(beta_overall, 2)
```


### 90-day mortality

From `nrow(dat)` patients in our *Observe-ACS* cohort we observed overall mortality in `r sum(dat.outcome$V_mortality_90d==1)` patients whereas `r sum(dat.outcome$V_mortality_90d==0)` survived. In overall mortality we observed `r sum(dat.outcome$V_mortality_90d==1)`, so our number of events has more than halved. Let's see what we get: 

```{r elastic-net-90d-mortality, echo=FALSE}

set.seed(20220106)
# for cross-validation
K=5

# impdat %>%
#   # only interested in pat without Afib, since that is an indication for NOAC per se
#   filter(basis_afib == "0") %>%
#   # drop column, since no information (only zeros)
#   select(-basis_afib) -> dat2

ctrl <- caret::trainControl(method = "cv",
                     number = K,  
                     classProbs = T,
                     summaryFunction = caret::twoClassSummary)

enet_grid <- expand.grid(alpha = seq(0.1, 0.9, 0.1),
                         lambda = c(0.1, 0.2, 0.5))

# create matrix to store beta values with nrow=m, and ncol=ncol(independent_data)+auc
beta_90 <- matrix(nrow = m, ncol = ncol(impdat) - 2)  # we do not need .imp, .id, V_rapID in our matrix (but we will add "Best_AUC")
colnames(beta_90) <- c(names(impdat[,-c(1,2,3)]), "Best_AUC")

# iterate over all multiple imputed datasets 1:m ---------------------------
tictoc::tic()
for(i in 1:m) {
  ## independent data "X"
  impdat %>% 
    dplyr::filter(.imp == i) %>%  # filter imputed dataset 
    # drop unnecessary cols (decide what we want to model)
    dplyr::select(-c(.id, .imp, V_rapID)) %>% 
    # convert to data.matrix necessary for caret
    base::data.matrix(.) %>% 
    # scale data
    base::scale(x=., center=TRUE, scale=TRUE) -> X
          # .imp An optional column number or column name in long, indicating the imputation
          # index. The values are assumed to be consecutive integers between 0 and m.
          # Values 1 through m correspond to the imputation index, value 0 indicates the
          # original data (with missings). By default, the procedure will search for a variable
          # named ".imp".
  ## target data "y"
  dat.outcome %>% 
    # we do not have multiple imputed outcome data (no need to filter(.imp == i))
    select(V_mortality_90d) %>%
    # unlist needed for caret
    unlist %>%
    # rename factor labels
    factor(., labels = c("no", "yes")) -> y
  
  plr1 <- caret::train(x=X, 
                       y=y, 
                       method = "glmnet", 
                       metric = "ROC", 
                       tuneGrid = enet_grid,
                       trControl = ctrl)
  
  net1 <- glmnet(X, 
                 as.factor(y),
                 family = "binomial",
                 alpha = plr1$bestTune$alpha, # tune!
                 lambda = plr1$bestTune$lambda) # tune!
  
  beta_90[i, ] <- c(as.numeric(net1$beta), max(plr1$results$ROC))
}
tictoc::toc()

# add mean of all imputation values, min, max and colSums where beta !=0
beta_90 <- rbind(beta_90,  # beta values (for m imputations m rows)
              colMeans(beta_90),  # mean of beta values across imputations
              apply(beta_90, 2, min),  # min beta across imputations
              apply(beta_90, 2, max),  # max beta across imputations
              colSums(beta_90 != 0)  # how often was feature chosen? so where is beta not null and sumup
              )

beta_90 <- base::t(beta_90)  # matrix transpose
# order matrix based on how hoften betas were calculated
index <- rev(
  order(  # Ordering Permutation -> returns index
    beta_90[,ncol(beta_90)]  # order based on last col of beta (which is how often a feature was chosen (has beta values))
    )
  )
# order beta matrix
beta_90 <- beta_90[index,]
# delete all the beta estimates based on one imputation only
beta_90 <- beta_90[,-(1:m)]
colnames(beta_90) <- c("Mean Estimate", "Minimum", "Maximum", "Selection Count") # count how often each feature was selected in imputed datasets 
```

We calculate our estimates based on `r length(unique(impdat$.id))` patients in the ACS observe zone.  

We obtain

```{r beta-90d-mortality, echo=FALSE}
round(beta_90, 2)
```


### 30-day mortality

From `nrow(dat)` patients in our *Observe-ACS* cohort we observed overall mortality in `r sum(dat.outcome$V_mortality_30d==1)` patients whereas `r sum(dat.outcome$V_mortality_30d==0)` survived. So we really have only a few events in that short 30-day period... 

```{r elastic-net-30d-mortality, echo=FALSE, warning=FALSE}

set.seed(20220106)
# for cross-validation
K=5

# impdat %>%
#   # only interested in pat without Afib, since that is an indication for NOAC per se
#   filter(basis_afib == "0") %>%
#   # drop column, since no information (only zeros)
#   select(-basis_afib) -> dat2

ctrl <- caret::trainControl(method = "cv",
                     number = K,  
                     classProbs = T,
                     summaryFunction = caret::twoClassSummary)

enet_grid <- expand.grid(alpha = seq(0.1, 0.9, 0.1),
                         lambda = c(0.1, 0.2, 0.5))

# create matrix to store beta values with nrow=m, and ncol=ncol(independent_data)+auc
beta_30 <- matrix(nrow = m, ncol = ncol(impdat) - 2)  # we do not need .imp, .id, V_rapID in our matrix (but we will add "Best_AUC")
colnames(beta_30) <- c(names(impdat[,-c(1,2,3)]), "Best_AUC")

# iterate over all multiple imputed datasets 1:m ---------------------------
tictoc::tic()
for(i in 1:m) {
  ## independent data "X"
  impdat %>% 
    dplyr::filter(.imp == i) %>%  # filter imputed dataset 
    # drop unnecessary cols (decide what we want to model)
    dplyr::select(-c(.id, .imp, V_rapID)) %>% 
    # convert to data.matrix necessary for caret
    base::data.matrix(.) %>% 
    # scale data
    base::scale(x=., center=TRUE, scale=TRUE) -> X
          # .imp An optional column number or column name in long, indicating the imputation
          # index. The values are assumed to be consecutive integers between 0 and m.
          # Values 1 through m correspond to the imputation index, value 0 indicates the
          # original data (with missings). By default, the procedure will search for a variable
          # named ".imp".
  ## target data "y"
  dat.outcome %>% 
    # we do not have multiple imputed outcome data (no need to filter(.imp == i))
    select(V_mortality_30d) %>%
    # unlist needed for caret
    unlist %>%
    # rename factor labels
    factor(., labels = c("no", "yes")) -> y
  
  plr1 <- caret::train(x=X, 
                       y=y, 
                       method = "glmnet", 
                       metric = "ROC", 
                       tuneGrid = enet_grid,
                       trControl = ctrl)
  
  net1 <- glmnet(X, 
                 as.factor(y),
                 family = "binomial",
                 alpha = plr1$bestTune$alpha, # tune!
                 lambda = plr1$bestTune$lambda) # tune!
  
  beta_30[i, ] <- c(as.numeric(net1$beta), max(plr1$results$ROC))
}
tictoc::toc()

# add mean of all imputation values, min, max and colSums where beta !=0
beta_30 <- rbind(beta_30,  # beta values (for m imputations m rows)
              colMeans(beta_30),  # mean of beta values across imputations
              apply(beta_30, 2, min),  # min beta across imputations
              apply(beta_30, 2, max),  # max beta across imputations
              colSums(beta_30 != 0)  # how often was feature chosen? so where is beta not null and sumup
              )

beta_30 <- base::t(beta_30)  # matrix transpose
# order matrix based on how hoften betas were calculated
index <- rev(
  order(  # Ordering Permutation -> returns index
    beta_30[,ncol(beta_30)]  # order based on last col of beta (which is how often a feature was chosen (has beta values))
    )
  )
# order beta matrix
beta_30 <- beta_30[index,]
# delete all the beta estimates based on one imputation only
beta_30 <- beta_30[,-(1:m)]
colnames(beta_30) <- c("Mean Estimate", "Minimum", "Maximum", "Selection Count") # count how often each feature was selected in imputed datasets 
```

We calculate our estimates based on `r length(unique(impdat$.id))` patients in the ACS observe zone.  

We obtain

```{r beta-30d-mortality, echo=FALSE}
round(beta_30, 2)
```

\clearpage

# Machine Learning

### Prepare dataset

We use all variables that were selected at least **x** times (we can increase m up to 100 times, but will do that only once).

```{r ml-data, message=FALSE}
# select all vars that were selected over threshold without col "Best_AUC"
inds <- setdiff(
  rownames(beta_overall)[which(beta_overall[,"Selection Count"]>=50)],  # features that were selected over threshold (and AUC which is reported in all imputations..)
  "Best_AUC"
  )

# reduce dataset vars so that only selected features (inds) are used
impdat %>% 
  select(.id, dplyr::all_of(inds)) %>%
  mutate(.id = factor(.id)) -> impdat.ml

# create empty matrix with nrow= no of patients, and ncol= no of selected features
impdat.ml.mean <- matrix(nrow = length(levels(impdat.ml$.id)), ncol = ncol(impdat.ml))
# convert to dataframe
impdat.ml.mean <- data.frame(impdat.ml.mean)
colnames(impdat.ml.mean) <- colnames(impdat.ml)

## dat3 = impdat.ml
##dat4 = impdat.ml.mean

# build "mean/mode" of imputed dataset   (not recommended by developers of mice..)
# CAVE: factors in R -> now factor encoding becomes a bit tricky 0 -> 1, and 1->2 (this is because R stores factors by default with 1 and 2, type `str(categorical_var)` to see how the factor was encoded)
for(i in 1:length(levels(impdat.ml$.id))) {  # 1:961
  impdat.ml %>%
    filter(.id == levels(impdat.ml$.id)[i]) %>%
    select(-.id) -> dummy  # all imputed datasets of one individuum with nrows=m
  impdat.ml.mean[i, 1]  <- levels(impdat.ml$.id)[i]
  impdat.ml.mean[i, -1] <- sapply(dummy, function(x) {
    if (is(x, "numeric")) 
        res <- mean(x)
    else if (is(x, "factor"))
      res <- mode(x)  # factors change here
    else 
      res <- NA
    return(res)
  })
}

# Scale independent data (better performed only on training data...)
impdat.ml.mean %>% 
  select(-.id) %>% 
  data.matrix %>% 
  scale(x=., center=TRUE, scale=TRUE) -> X

## target data "y"
dat.outcome %>%
  # we do not have multiple imputed outcome data (no need to filter(.imp == i))
  select(V_o_mortality) %>%
  # unlist needed for caret
  unlist %>%
  # rename factor labels
  factor(., labels = c("no", "yes")) -> y
```


## Overall Mortality

In the following, we do `r K`-fold cross-validation to appropriately evaluate
the models.

We have `r nrow(X)` patients and `r ncol(X)` variables.
`r sum(y == "yes")` patients had a stroke and `r sum(y == "no")` had not.


### Logistic Regression

```{r logreg, echo=FALSE}
set.seed(2022)

logreg <- caret::train(X, 
                       y, 
                       method = "glm", 
                       metric = "ROC", 
                       trControl = ctrl)

# https://www.statology.org/glm-fit-fitted-probabilities-numerically-0-or-1-occurred/
```


We obtain an AUC value of `r logreg$results$ROC`.

## Random Forest

```{r rf}
set.seed(2022)

grid_rf <- expand.grid(mtry = seq(1, ncol(X), 1))

randfor <- caret::train(X, 
                        y, 
                        method = "rf", 
                        metric = "ROC", 
                        tuneGrid = grid_rf,
                        trControl = ctrl)

l <- which.max(randfor$results$ROC)
```

Using a tuning parameter of `mtry = ` `r randfor$results$mtry[l]`, 
we obtain an AUC of `r randfor$results$ROC[l]`.

## Support Vector Machine

```{r svm, message=FALSE}
set.seed(463)

grid_svm <- expand.grid(C = c(.1, 1, 5),
                        sigma = c(.05, .1, .5, 1))

svm_res <- caret::train(X, 
                        y, 
                        method = "svmRadial", 
                        metric = "ROC", 
                        tuneGrid = grid_svm,
                        trControl = ctrl)

l <- which.max(svm_res$results$ROC)
```

With tuning parameters `sigma = ` `r svm_res$results$sigma[l]` and
`C = ` `r svm_res$results$C[l]`, we observe an AUC value of `r svm_res$results$ROC[l]`.


## Naive Bayes

```{r nb, message=FALSE, warning=FALSE}
set.seed(435)

nb_res <- caret::train(X, 
                       y, 
                       method = "nb", 
                       metric = "ROC", 
                       trControl = ctrl)
```

We obtain an AUC value of `r max(nb_res$results$ROC)`.


\clearpage


# ROC Curves

## Logistic Regression

```{r plot-auc-logreg}
library(pROC)

pROC_logreg_overall <- roc(y, predict(logreg, X, type="prob")[,"yes"],
            smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)

sens.ci <- ci.se(pROC_logreg_overall)
plot(sens.ci, type="shape", col="lightblue")

# precision/ recall
library(precrec)  # nice package
precrec_logreg_overall <- precrec::evalmod(scores = predict(logreg, X, type="prob")[,"yes"], 
                                    labels = y,
                                     mode="basic"  # default mode="rocprc"
                                    )
autoplot(precrec_logreg_overall)
```


## RF 

Here, we draw an ROC curve for the random forest.

```{r plot-auc-rf, message=FALSE}
pROC_rf_overall <- roc(y, randfor$finalModel$votes[,"yes"],
            smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)

sens.ci <- ci.se(pROC_rf_overall)
plot(sens.ci, type="shape", col="lightblue")

# precision/ recall
library(precrec)  # nice package
precrec_rf_overall <- precrec::evalmod(scores = randfor$finalModel$votes[,"yes"], 
                                    labels = y,
                                     mode="basic"  # default mode="rocprc"
                                    )
autoplot(precrec_rf_overall)
```


\clearpage


# Software

This analysis was carried out using the statistical software `r base::version$version.string`.

Apart from R's base functionality, the following packages were used: <br/>

```{r software, echo=FALSE}
# or faster with function...
installed.packages()[names(sessionInfo()$otherPkgs), "Version"]
```

