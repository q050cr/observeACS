---
title: "Observe-ACS | Model Copeptin"
author: | 
        | christoph.reich@med.uni-heidelberg.de 
        | evangelos.giannitsis@med.uni-heidelberg.de
        | Klinik für Kardiologie, Angiologie und Pneumologie
        | Universitätsklinikum Heidelberg
        | 
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    df_print: kable
---


```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(
  echo = FALSE)
```

```{r dependencies, include=FALSE}
library(readxl)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(tableone)
library(skimr)
library(mice)  # for missing data
library(VIM)  # for missing data
library(caret)  # for modelling
library(tictoc)
library(pROC)
library(glmnet)
library(survival)
library(survminer)
library(ggsci)
library(lubridate)

# define mode function (will be needed later)  !!!
mode <- function(x, na.rm = FALSE) {
  if(na.rm){
    x = x[!is.na(x)]
  }

  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}
```


```{r read-data, echo=FALSE}
# ran on cluster 100 mice repetitions
impdat.ml.mean <- readRDS(file="../data/impdat_ml_mean_m100.rds")
impdat <- readRDS(file="../data/impdat_m100.RDS")

base::source("../scripts/data_preprocessing.R")
```

![AG Meder](../../img/meder_presentation_heart.png)


# Model with Copeptin

```{r copeptin-data, echo=FALSE}
m=100
index=which(!is.na(dat.observe$COPEPTIN_COMBINED))
# pull complete observations of copeptin + ID from all data (before mice and dropping cols with NA>30%)
copeptin_data <- dat.observe[index, c("V_rapID", "COPEPTIN_COMBINED")]

# indices checked, after mice and mean imputation of all datasets we can again use the same indices to combine data
copeptin_data1 <- impdat.ml.mean[index, ]
# colbind
copeptin_data <- cbind(copeptin_data1, copeptin_data["COPEPTIN_COMBINED"])

# outcome data after imputation
dat.outcome_copeptin <- dat.outcome[index, ]


# FOR FEATURE SELECTION:
## if we want to perform the same feature selection as before, we have to replicate our vector first
mice_index <- c(index)
for (i in seq_along(1:(m-1))) {
  # we imputed m times, therefore we need to concat our vector 
  mice_index <- c(mice_index, index+ nrow(dat.observe)*i)
}

# select copeptin data (m*times)
mice_copeptin <- impdat[mice_index, ]
# since we imputed m*times, we also have to replicate our copeptin vector m times
mice_copeptin["COPEPTIN_COMBINED"] <- rep(copeptin_data$COPEPTIN_COMBINED, m)
```

**Copeptin** on top of cardiac troponin supports safe discharge in patients with chest pain or other symptoms suggestive of ACS under routine conditions with the use of a broad spectrum of local standard POC, conventional and high-sensitivity troponin assays. Here we want to investigate if copeptin increases performance of our prognostic models. Unfortunately, within our `observeACS` cohort **copeptin** was only measured in `r nrow(copeptin_data)` patients. There were 


In those `r nrow(copeptin_data)` patients in our *Observe-ACS* cohort we observed overall mortality in only `r sum(dat.outcome_copeptin$V_o_mortality==1)` patients whereas `r sum(dat.outcome_copeptin$V_o_mortality==0)` survived. The low number of events definitely worsens our predictive possibilities.. 


## Lasso regression

Sample split p=0.6 for train/test set:

```{r more-classical-stats, echo=FALSE}
library(glmnet)
# lasso regression --------------------------------------------------------
set.seed(2022)
dat.outcome_copeptin %>% 
  # we do not have multiple imputed outcome data (no need to filter(.imp == i))
  select(V_o_mortality) %>%
  # unlist needed for caret
  unlist %>%
  # rename factor labels
  factor(., labels = c("no", "yes")) -> y

# split data
in.train <- createDataPartition(y, p=0.6, list=FALSE)
# look at how many events are in each group..
ytrain <- y[in.train]; summary(factor(ytrain))
ytest <- y[-in.train]; summary(factor(ytest))

# create data to model and drop some cols
modeldat <- tibble::as_tibble(cbind(dat.outcome_copeptin["V_o_mortality"], copeptin_data)) %>% 
  select(-c(.id, V_t0_quick_value, V_t0_hkt_value, V_t0_hb_value, V_t0_ldh_value, V_t0_na_value))

# split data
train.data <- modeldat[as.vector(in.train),]
test.data <- modeldat[-as.vector(in.train),]

# additional data preparation
# Dumy code categorical predictor variables
x <- model.matrix(V_o_mortality~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- ifelse(train.data$V_o_mortality == "0", 0, 1)

# lasso model

# Find the best lambda using cross-validation
# cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial", nfolds = 3)  # we receive errors here, too little observations

# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "binomial",
                lambda = 0.054 #cv.lasso$lambda.min   ## here random penalty used
                )
# Display regression coefficients
coef(model)
## Copeptin dropped by lasso regression (can be seen as feature selection)

# Make predictions on the test data
x.test <- model.matrix(V_o_mortality ~., test.data)[,-1]
# get probs
probabilities <- model %>% predict(newx = x.test, type="response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)

# Model accuracy
observed.classes <- test.data$V_o_mortality
mean(predicted.classes == observed.classes)

# we cant use accuracy as a measure in that setting --> we need another threshold
library(pROC)
pROC_lasso_copeptin <- roc(observed.classes, probabilities,  # or 1-probabilities
                           smoothed = TRUE,
                           # arguments for ci
                           ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                           # arguments for plot
                           plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                           print.auc=TRUE, show.thres=TRUE)

# precision/ recall
library(precrec)  # nice package
precrec_logreg_overall <- precrec::evalmod(scores = probabilities, 
                                           labels = observed.classes,
                                           mode="basic"  # default mode="rocprc"
)
autoplot(precrec_logreg_overall)
```

As seen above, we do not obtain reasonable results due to little events and a small cohort size. Furthermore, we can see that there were no signs that Copeptin would have been chosen in a model above. <br/>

## Logistic Regression

Now we hit it without train/test split and try some simple logistic regression models (all with copeptin).


```{r more-classical-stats2, echo=FALSE}
#------------------------------------------------#
# logistic regression without training data -------------------------------
#------------------------------------------------#

m.01 <- glm(V_o_mortality~COPEPTIN_COMBINED+V_t0_hstnt_value+KHK__Killip_Class+V_age, data=modeldat, 
                     family = binomial(link="logit")
)

m.02 <- glm(V_o_mortality~COPEPTIN_COMBINED+V_t0_hstnt_value+V_age, data=modeldat, 
            family = binomial(link="logit")
)

m.03 <- glm(V_o_mortality~COPEPTIN_COMBINED+V_age, data=modeldat, 
            family = binomial(link="logit")
)

m.04 <- glm(V_o_mortality~V_age, data=modeldat, 
            family = binomial(link="logit")
)

summary(m.01)
summary(m.02)
summary(m.03)
summary(m.04)

anova(m.01, m.02,  test="Chisq")
anova(m.01, m.03, test="Chisq")

m.01_probabilities <- predict(m.01, newdata = modeldat[-1], type = "response")
m.02_probabilities <- predict(m.02, newdata = modeldat[-1], type = "response")
m.03_probabilities <- predict(m.03, newdata = modeldat[-1], type = "response")
m.04_probabilities <- predict(m.04, newdata = modeldat[-1], type = "response")

ind<-modeldat$V_o_mortality==1
m.03_probabilities[ind]

  library(pROC)
pROC_m.01 <- roc(modeldat$V_o_mortality, m.01_probabilities,  # or 1-probabilities
                           smoothed = TRUE,
                           # arguments for ci
                           ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                           # arguments for plot
                           plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                           print.auc=TRUE, show.thres=TRUE)

pROC_m.02 <- roc(modeldat$V_o_mortality, m.02_probabilities,  # or 1-probabilities
                 smoothed = TRUE,
                 # arguments for ci
                 ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                 # arguments for plot
                 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                 print.auc=TRUE, show.thres=TRUE)

pROC_m.03 <- roc(modeldat$V_o_mortality, m.03_probabilities,  # or 1-probabilities
                 smoothed = TRUE,
                 # arguments for ci
                 ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                 # arguments for plot
                 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                 print.auc=TRUE, show.thres=TRUE)

pROC_m.04 <- roc(modeldat$V_o_mortality, m.04_probabilities,  # or 1-probabilities
                 smoothed = TRUE,
                 # arguments for ci
                 ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                 # arguments for plot
                 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                 print.auc=TRUE, show.thres=TRUE)

```

Regarding our logistic regression models, it seems that all information on overall mortality comes from `age` (with regard to the selected variables above). Next, we will continue to build Survival models where we have additional time2event data for mortality. 

\clearpage


# Software

This analysis was carried out using the statistical software `r base::version$version.string`.

Apart from R's base functionality, the following packages were used: <br/>

```{r software, echo=FALSE}
# or faster with function...
installed.packages()[names(sessionInfo()$otherPkgs), "Version"]
```
